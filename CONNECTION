# ===============================
# Install required packages (Colab only)
# ===============================
!pip install fastapi nest_asyncio pyngrok uvicorn transformers torch --quiet

# ===============================
# Imports
# ===============================
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import nest_asyncio
from pyngrok import ngrok
import uvicorn
import torch
from google.colab import userdata
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ===============================
# Fix event loop for Colab
# ===============================
nest_asyncio.apply()

# ===============================
# Initialize FastAPI app
# ===============================
app = FastAPI()

# Allow frontend access
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===============================
# Load Hugging Face FLAN-T5 model
# ===============================
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# ===============================
# Pydantic models
# ===============================
class Message(BaseModel):
    message: str

class QuestionRequest(BaseModel):
    role: str
    mode: str
    domain: str = None

# ===============================
# Chat endpoint
# ===============================
@app.post("/chat")
async def chat(msg: Message):
    user_message = msg.message.lower()

    reply = "Interesting! Can you elaborate more?" # Default reply

    if "generate question" in user_message:
        # Default example question
        prompt = "Generate a technical interview question for the role of Software Engineer."
        try:
            inputs = tokenizer(prompt, return_tensors="pt")
            outputs = model.generate(
                **inputs,
                max_length=100,
                temperature=0.7,
                do_sample=True,
                top_p=0.9
            )
            reply = tokenizer.decode(outputs[0], skip_special_tokens=True)
        except Exception as e:
            logger.error(f"Error generating question in chat endpoint: {e}")
            reply = "Sorry, I encountered an error while generating the question."

    elif "hello" in user_message or "hi" in user_message:
        reply = "Hello! How can I assist you with your interview preparation?"
    elif "strength" in user_message:
        reply = "I usually highlight my problem-solving skills and adaptability."
    elif "weakness" in user_message:
        reply = "I work on improving time management to perform better."

    return {"reply": reply}

# ===============================
# Generate interview question endpoint
# ===============================
@app.post("/generate-question")
async def generate_question_api(req: QuestionRequest):
    prompt = f"Generate a {req.mode} interview question for the role of {req.role}"
    if req.domain:
        prompt += f" in the domain of {req.domain}"
    prompt += "."

    try:
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(
            **inputs,
            max_length=100,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )
        question = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return {"question": question}
    except Exception as e:
        logger.error(f"Error generating question in generate-question endpoint: {e}")
        return {"error": "Sorry, I encountered an error while generating the question."}

# ===============================
# Start ngrok tunnel
# ===============================
# Replace this with your Colab secret NGROK token if needed
NGROK_AUTH_TOKEN = userdata.get('backend')

if NGROK_AUTH_TOKEN:
    ngrok.set_auth_token(NGROK_AUTH_TOKEN)

try:
    public_url = ngrok.connect(8000).public_url
    print("ðŸš€ Public URL:", public_url)
except Exception as e:
    logger.error(f"Error starting ngrok tunnel: {e}")
    print("Failed to start ngrok tunnel.")


# ===============================
# Run Uvicorn server
# ===============================
if __name__ == "__main__":
    try:
        uvicorn.run(app, host="0.0.0.0", port=8000, reload=False)
    except Exception as e:
        logger.error(f"Error running uvicorn server: {e}")
        print("Failed to start uvicorn server.")
